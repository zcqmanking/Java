<?xml version="1.0" encoding="UTF-8"?><crawl-order xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="heritrix_settings.xsd">
  <meta>
    <name>jdBook</name>
    <description>jdBookProfile</description>
    <operator>Admin</operator>
    <organization/>
    <audience/>
    <date>20120203162400</date>
  </meta>
  <controller>
    <string name="settings-directory">settings</string><!-- Heritrix的顶级目录 -->
    <string name="disk-path"/><!-- order.xml所在目录,单个Heritrix实例的目录 -->
    <string name="logs-path">logs</string><!-- 用于保存Heritrix的日志文件,可以是绝对路径,也可以是相对路径,相对路径是相对于disk-path-->
    <string name="checkpoints-path">checkpoints</string><!-- 用于保存checkpoints(定点备份)文件的目录, 可以是绝对路径,也可以是相对路径,相对路径是相对于disk-path-->
    <string name="state-path">state</string><!-- 用于保存crawler-state文件的目录,可以是绝对路径,也可以是相对路径,相对路径是相对于disk-path -->
    <string name="scratch-path">scratch</string><!-- 用于保存网页内容临时文件的目录,可以是绝对路径,也可以是相对路径,相对路径是相对于disk-path-->
    <long name="max-bytes-download">0</long><!-- 最大下载字节数，当下载字节超出该值爬虫将停止下载。如果该值为0则表示没有限制-->
    <long name="max-document-download">0</long><!-- 最大文档下载数，当下载文档超出该值时爬虫将停止下载。如果该值为0则表示没有限制-->
    <long name="max-time-sec">0</long><!-- 最大时间抓取(秒),如果抓取时间超过该值，则爬虫将停止抓取。如果该值为0则表示没有限制-->
    <integer name="max-toe-threads">50</integer><!-- 最大线程数,用于同时处理多个URI-->
    <integer name="recorder-out-buffer-bytes">4096</integer><!-- 每一个线程的输出缓冲区大小,也就是在内存里存放多大的字节数才写入到文件中-->
    <integer name="recorder-in-buffer-bytes">65536</integer><!-- 每一个线程的输入缓冲区大小,也就是在内存里存放多大的字节数才写入到文件中-->
    <integer name="bdb-cache-percent">0</integer><!--分配给BDB缓存堆的百分比,默认为0则表示没有其他要求(通常BDB是需要60%或者是最大值) -->
    <newObject name="scope" class="org.archive.crawler.deciderules.DecidingScope"><!-- 抓取范围，构造CrawlScope-->
      <boolean name="enabled">true</boolean> <!-- 是否运行这个组件 -->
      <string name="seedsfile">seeds.txt</string> <!--种子文件名-->
      <boolean name="reread-seeds-on-config">true</boolean><!-- 是否每一个配置发生变化都要引发重新读取原始种子文件 -->
      <newObject name="decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence"><!--抓取范围限定的规则 -->
        <map name="rules"><!-- 不同的规则-->
          <newObject name="rejectByDefault" class="org.archive.crawler.deciderules.RejectDecideRule">
          </newObject>
          <newObject name="acceptIfSurtPrefixed" class="org.archive.crawler.deciderules.SurtPrefixedDecideRule">
            <string name="decision">ACCEPT</string>
            <string name="surts-source-file"/><!--用于推断SURT前缀的文件,文件里的任何文件将转换为所提供的SURT前缀,显示在行里的SURT前缀都会通过+开始 -->
            <boolean name="seeds-as-surt-prefixes">true</boolean><!--种子文件是否也应当解析成SURT前缀 -->
            <string name="surts-dump-file"/><!--保存SURT前缀的文件，用于实际调试SURTS时 -->
            <boolean name="also-check-via">false</boolean><!--是否也检查该CrawlURI中的via -->
            <boolean name="rebuild-on-reconfig">true</boolean><!-- 当配置文件更改后,是否也跟着更改-->
          </newObject>
          <newObject name="rejectIfTooManyHops" class="org.archive.crawler.deciderules.TooManyHopsDecideRule">
            <integer name="max-hops">20</integer><!--最大跃点数,也就是抓取深度 -->
          </newObject>
          <newObject name="acceptIfTranscluded" class="org.archive.crawler.deciderules.TransclusionDecideRule">
            <integer name="max-trans-hops">3</integer><!--除去链接L，PathFromSeed的最大长度 -->
            <integer name="max-speculative-hops">1</integer><!--抽取的链接X，可能是链接L或者嵌入式E，在JS里的最大个数，通过pathFromSeed判断 -->
          </newObject>
          <newObject name="rejectIfPathological" class="org.archive.crawler.deciderules.PathologicalPathDecideRule">
            <integer name="max-repetitions">2</integer><!--一个URL相同目录段名最大重复次数，超过该值返回REJECT，如http://www.baidu.com/a/a/a/index.html ,其中/a出现三次，超过了2次，所以返回REJECT(拒绝)-->
          </newObject>
          <newObject name="rejectIfTooManyPathSegs" class="org.archive.crawler.deciderules.TooManyPathSegmentsDecideRule">
            <integer name="max-path-depth">20</integer><!--URL中段的次数是否超过该值，超过返回REJET，段表示http://www.baidu.com/a/b,其中a和b表示一个段 -->
          </newObject>
          <newObject name="acceptIfPrerequisite" class="org.archive.crawler.deciderules.PrerequisiteAcceptDecideRule">
          </newObject>
        </map>
      </newObject>
    </newObject>
    <map name="http-headers"><!-- HTTP协议，当处理爬虫HTTP协议时需要构造 -->
      <string name="user-agent">Mozilla/5.0 (compatible; heritrix/1.14.4+http://book.360buy.com/)</string><!-- 用户代理，这个值字段必须包含有效的URL，如此才可以用爬虫访问个人或者组织的网站 -->
      <string name="from">abc@abc.com</string><!--联系人信息,该字段必须包含有效的email，格式正确即可，来代表使用本爬虫的个人或组织 -->
    </map>
    <newObject name="robots-honoring-policy" class="org.archive.crawler.datamodel.RobotsHonoringPolicy"><!--Robots.txt协议控制 --> 
      <string name="type">classic</string><!-- 爬虫协议类型，有classic,ignore,custom,most-favored,most-favored-set 5种类型 -->
      <boolean name="masquerade">false</boolean><!-- 我们应当在当爬虫遵循所有它声明的规则时伪装另一个代理，唯一相关的类型是：most-favored和most-favored-set-->
      <text name="custom-robots"/><!-- 如果type是custom，则机器人自定义-->
      <stringList name="user-agents"><!-- 如果type是most-favored-set，代替的user-agents，这里列表多个 -->
      </stringList>
    </newObject>
    <newObject name="frontier" class="org.archive.crawler.frontier.BdbFrontier"><!-- Frontier 调度器 -->
      <float name="delay-factor">1.0</float><!-- 从同一个服务器(host)获取需要等待的间隔时间,可以预防无节制的抓取一个网站.通常是用该值去乘以上一个url的抓取时间来表示为下一个url需要等待的时间 -->
      <integer name="max-delay-ms">2000</integer><!-- 最大的等待时间,单位毫秒 -->
      <integer name="min-delay-ms">0</integer><!-- 最小等待时间,单位毫秒-->
      <integer name="respect-crawl-delay-up-to-secs">300</integer><!--当读取robots.txt时推迟抓取的时间，单位毫秒 -->
      <integer name="max-retries">5</integer><!-- 已经尝试失败的URI的重新尝试次数,很多人在跑Heritrix的时候，发现只跑了30个URL就停止了,其实是一个URL都没成功，它这里重试了30次 -->
      <long name="retry-delay-seconds">900</long><!--默认多长时间我们重新去抓取一个检索失败的URI -->
      <integer name="preference-embed-hops">1</integer><!--嵌入或者重定向URI调度等级，例如，该值为1(默认也为1)，调度时将比普通的link等级高.如果设置为0，则和link一样 -->
      <integer name="total-bandwidth-usage-KB-sec">0</integer><!--爬虫所允许的最大宽带平均数，实际的读取速度是不受此影响的，当爬虫使用的宽带接近极限时，它会阻碍新的URI去处理，0表示没有限制 -->
      <integer name="max-per-host-bandwidth-usage-KB-sec">0</integer><!--爬虫允许的每个域名所使用的最大宽带数，实际的读取速度不会受此影响，当爬虫使用的宽带接近极限时，它会阻碍新的URI去处理，0表示没有限制 -->
      <string name="queue-assignment-policy">org.archive.crawler.frontier.HostnameQueueAssignmentPolicy</string><!--定义如何去分配URI到各个队列,这个类是相同的host的url就属于同一个队列 -->
      <string name="force-queue-assignment"/><!--强制URI的队列名字 -->
      <boolean name="pause-at-start">false</boolean><!-- 在URI被尝试前，当爬虫启动后是否暂停？这个操作可以在爬虫工作前核实或调整爬虫。默认为false -->
      <boolean name="pause-at-finish">false</boolean><!-- 当爬虫结束时是否暂停，而不是立刻停止工作.这个操作可以在爬虫状态还是可用时，有机会去显示爬虫结果,并有可能去增加URI和调整setting，默认为false-->
      <boolean name="source-tag-seeds">false</boolean><!-- 是否去标记通过种子抓取的uri作为种子的遗传，用source值代替.-->
      <boolean name="recovery-log-enabled">true</boolean><!--设置为false表示禁用恢复日志写操作，为true时候表示你用checkpoint去恢复crawl销毁的数据 -->
      <boolean name="hold-queues">true</boolean><!--当队列数量未达到时，是否不让其运行，达到了才运行。是否要去持久化一个创建的每个域名一个的URI工作队列直到他们需要一直繁忙(开始工作)。如果为false(默认值)，队列会在任何时间提供URI去抓取。如果为true，则队列一开始(还有收集的url)会处于不在活动中的状态,只有在Frontier需要另外一个队列使得所有线程繁忙的时候才会让一个新的队列出于活动状态. -->
      <integer name="balance-replenish-amount">3000</integer><!--补充一定的数量去使得队列平衡，更大的数目则意味着更多的URI将在它们处于等待队列停用之前将被尝试 -->
      <integer name="error-penalty-amount">100</integer>
      <long name="queue-total-budget">-1</long>
      <string name="cost-policy">org.archive.crawler.frontier.ZeroCostAssignmentPolicy</string>
      <long name="snooze-deactivate-ms">300000</long>
      <integer name="target-ready-backlog">200</integer>
      <string name="uri-included-structure">org.archive.crawler.util.BdbUriUniqFilter</string>
      <boolean name="dump-pending-at-close">false</boolean>
    </newObject>
    <map name="uri-canonicalization-rules"><!-- URL规范化规则，URL规范化规则有序列表，规则适用于从上至下列出的顺序-->
      <newObject name="Lowercase" class="org.archive.crawler.url.canonicalize.LowercaseRule">
        <boolean name="enabled">true</boolean>
      </newObject>
      <newObject name="Userinfo" class="org.archive.crawler.url.canonicalize.StripUserinfoRule">
        <boolean name="enabled">true</boolean>
      </newObject>
      <newObject name="WWW[0-9]*" class="org.archive.crawler.url.canonicalize.StripWWWNRule">
        <boolean name="enabled">true</boolean>
      </newObject>
      <newObject name="SessionIDs" class="org.archive.crawler.url.canonicalize.StripSessionIDs">
        <boolean name="enabled">true</boolean>
      </newObject>
      <newObject name="SessionCFIDs" class="org.archive.crawler.url.canonicalize.StripSessionCFIDs">
        <boolean name="enabled">true</boolean>
      </newObject>
      <newObject name="QueryStrPrefix" class="org.archive.crawler.url.canonicalize.FixupQueryStr">
        <boolean name="enabled">true</boolean>
      </newObject>
    </map>
    <map name="pre-fetch-processors"><!--预先处理链，在抓取前需要从网络获取或配置相关参数 -->
      <newObject name="Preselector" class="org.archive.crawler.prefetch.Preselector">
        <boolean name="enabled">true</boolean>
        <newObject name="Preselector#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <boolean name="override-logger">false</boolean>
        <boolean name="recheck-scope">true</boolean>
        <boolean name="block-all">false</boolean>
        <string name="block-by-regexp"/>
        <string name="allow-by-regexp"/>
      </newObject>
      <newObject name="Preprocessor" class="org.archive.crawler.prefetch.PreconditionEnforcer">
        <boolean name="enabled">true</boolean>
        <newObject name="Preprocessor#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <integer name="ip-validity-duration-seconds">0</integer>
        <integer name="robot-validity-duration-seconds">0</integer>
        <boolean name="calculate-robots-only">false</boolean>
      </newObject>
    </map>
    <map name="fetch-processors"><!-- 获取链 -->
      <newObject name="DNS" class="org.archive.crawler.fetcher.FetchDNS">
        <boolean name="enabled">true</boolean>
        <newObject name="DNS#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <boolean name="accept-non-dns-resolves">false</boolean>
        <boolean name="digest-content">true</boolean>
        <string name="digest-algorithm">sha1</string>
      </newObject>
      <newObject name="HTTP" class="org.archive.crawler.fetcher.FetchHTTP">
        <boolean name="enabled">true</boolean>
        <newObject name="HTTP#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <newObject name="midfetch-decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <integer name="timeout-seconds">1200</integer>
        <integer name="sotimeout-ms">20000</integer>
        <integer name="fetch-bandwidth">0</integer>
        <long name="max-length-bytes">0</long>
        <boolean name="ignore-cookies">false</boolean>
        <boolean name="use-bdb-for-cookies">true</boolean>
        <string name="load-cookies-from-file"/>
        <string name="save-cookies-to-file"/>
        <string name="trust-level">open</string>
        <stringList name="accept-headers">
        </stringList>
        <string name="http-proxy-host"/>
        <string name="http-proxy-port"/>
        <string name="default-encoding">ISO-8859-1</string>
        <boolean name="digest-content">true</boolean>
        <string name="digest-algorithm">sha1</string>
        <boolean name="send-if-modified-since">true</boolean>
        <boolean name="send-if-none-match">true</boolean>
        <boolean name="send-connection-close">true</boolean>
        <boolean name="send-referer">true</boolean>
        <boolean name="send-range">false</boolean>
        <string name="http-bind-address"/>
      </newObject>
    </map>
    <map name="extract-processors"><!-- 抽取链 -->
      <newObject name="ExtractorHTTP" class="org.archive.crawler.extractor.ExtractorHTTP">
        <boolean name="enabled">true</boolean>
        <newObject name="ExtractorHTTP#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
      <newObject name="ExtractorHTML" class="org.archive.crawler.extractor.ExtractorHTML">
        <boolean name="enabled">true</boolean>
        <newObject name="ExtractorHTML#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <boolean name="extract-javascript">true</boolean>
        <boolean name="treat-frames-as-embed-links">true</boolean>
        <boolean name="ignore-form-action-urls">false</boolean>
        <boolean name="extract-only-form-gets">true</boolean>
        <boolean name="extract-value-attributes">true</boolean>
        <boolean name="ignore-unexpected-html">true</boolean>
      </newObject>
      <newObject name="ExtractorCSS" class="org.archive.crawler.extractor.ExtractorCSS">
        <boolean name="enabled">true</boolean>
        <newObject name="ExtractorCSS#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
      <newObject name="ExtractorJS" class="org.archive.crawler.extractor.ExtractorJS">
        <boolean name="enabled">true</boolean>
        <newObject name="ExtractorJS#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
      <newObject name="ExtractorSWF" class="org.archive.crawler.extractor.ExtractorSWF">
        <boolean name="enabled">true</boolean>
        <newObject name="ExtractorSWF#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
    </map>
    <map name="write-processors"><!--写链 -->
      <newObject name="Archiver" class="org.archive.crawler.writer.ARCWriterProcessor">
        <boolean name="enabled">true</boolean>
        <newObject name="Archiver#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <boolean name="compress">true</boolean>
        <string name="prefix">IAH</string>
        <string name="suffix">${HOSTNAME_ADMINPORT}</string>
        <long name="max-size-bytes">100000000</long>
        <stringList name="path">
          <string>arcs</string>
        </stringList>
        <integer name="pool-max-active">5</integer>
        <integer name="pool-max-wait">300000</integer>
        <long name="total-bytes-to-write">0</long>
        <boolean name="skip-identical-digests">false</boolean>
      </newObject>
    </map>
    <map name="post-processors"><!-- 请求链：清理URI和在URI范围内填充新的URI -->
      <newObject name="Updater" class="org.archive.crawler.postprocessor.CrawlStateUpdater">
        <boolean name="enabled">true</boolean>
        <newObject name="Updater#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
      <newObject name="LinksScoper" class="org.archive.crawler.postprocessor.LinksScoper">
        <boolean name="enabled">true</boolean>
        <newObject name="LinksScoper#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
        <boolean name="override-logger">false</boolean>
        <boolean name="seed-redirects-new-seed">true</boolean>
        <integer name="preference-depth-hops">-1</integer>
        <newObject name="scope-rejected-url-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
      <newObject name="Scheduler" class="org.archive.crawler.postprocessor.FrontierScheduler">
        <boolean name="enabled">true</boolean>
        <newObject name="Scheduler#decide-rules" class="org.archive.crawler.deciderules.DecideRuleSequence">
          <map name="rules">
          </map>
        </newObject>
      </newObject>
    </map>
    <map name="loggers"><!-- 统计跟踪链.统计跟踪模块，指定用于监视抓取和写日志，以及报告和提供信息给用户接口-->
      <newObject name="crawl-statistics" class="org.archive.crawler.admin.StatisticsTracker">
        <integer name="interval-seconds">20</integer>
      </newObject>
    </map>
    <string name="recover-path"/>
    <boolean name="checkpoint-copy-bdbje-logs">true</boolean>
    <boolean name="recover-retain-failures">false</boolean>
    <boolean name="recover-scope-includes">true</boolean>
    <boolean name="recover-scope-enqueues">true</boolean>
    <newObject name="credential-store" class="org.archive.crawler.datamodel.CredentialStore"><!--凭证存储,如登陆凭证 --> 
      <map name="credentials">
      </map>
    </newObject>
  </controller>
</crawl-order>
